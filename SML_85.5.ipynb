{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "ZTNhlR64gj1v"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "# Load the data\n",
        "train_df = pd.read_csv(\"train.csv\")\n",
        "test_df = pd.read_csv(\"test.csv\")\n",
        "\n",
        "# Separate the target variable\n",
        "y_train = train_df[\"category\"].to_numpy() \n",
        "X_train = train_df.drop([\"ID\", \"category\"], axis=1).to_numpy() \n",
        "X_test = test_df.drop([\"ID\"], axis=1).to_numpy() \n",
        "\n",
        "# # Apply feature engineering\n",
        "# for i in range(X_train.shape[1]):\n",
        "#     col = X_train[:, i]\n",
        "#     X_train = np.column_stack((X_train, col + col ** 3 + col ** 2))\n",
        "\n",
        "# # Standardize the data using z-score normalization\n",
        "# scaler = StandardScaler()\n",
        "# X_train = scaler.fit_transform(X_train)\n",
        "\n",
        "# # Standardize the data using z-score normalization\n",
        "# scaler = StandardScaler()\n",
        "# X_train_standardized = scaler.fit_transform(X_train)\n",
        "# X_train = X_train_standardized\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "4V7Jm5HqqkV8"
      },
      "outputs": [],
      "source": [
        "# Normalize the data using min-max scaling\n",
        "# scaler = MinMaxScaler()\n",
        "# X_train_normalized = scaler.fit_transform(X_train)\n",
        "# X_test_normalized = scaler.transform(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5h807yAp2CR",
        "outputId": "d715e46f-3818-4339-ad2d-79a9c5b8b950"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1205\n"
          ]
        }
      ],
      "source": [
        "# Fit the LOF algorithm on X_train\n",
        "lof = LocalOutlierFactor(n_neighbors=5, algorithm='ball_tree', metric='euclidean')\n",
        "outliers = lof.fit_predict(X_train)\n",
        "\n",
        "# Remove the outliers from X_train and y_train\n",
        "X_train = X_train[outliers == 1]\n",
        "y_train = y_train[outliers == 1]\n",
        "\n",
        "print(len(X_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujDkFvgImRXg",
        "outputId": "3aba1ab4-2688-455e-8b7b-491f9fb507aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 2000 Features:\n",
            "Index(['n2047', 'n2921', 'n546', 'n2907', 'n2901', 'n2890', 'n2889', 'n2877',\n",
            "       'n561', 'n566',\n",
            "       ...\n",
            "       'n349', 'n3155', 'n975', 'n2340', 'n1343', 'n2018', 'n857', 'n198',\n",
            "       'n3797', 'n188'],\n",
            "      dtype='object', length=2000)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/numpy/lib/function_base.py:2829: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.9/dist-packages/numpy/lib/function_base.py:2830: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n"
          ]
        }
      ],
      "source": [
        "# Calculate the correlation matrix\n",
        "corr_matrix = np.corrcoef(X_train, rowvar=False)\n",
        "\n",
        "# Get the correlations with the target variable\n",
        "corr_with_target = corr_matrix[:-1, -1]\n",
        "\n",
        "# Get the absolute values of the correlations\n",
        "abs_corr_with_target = np.abs(corr_with_target)\n",
        "\n",
        "# Sort the correlations in descending order\n",
        "sorted_corrs = np.argsort(abs_corr_with_target)[::-1]\n",
        "\n",
        "# Get the top n features\n",
        "n = 2000\n",
        "top_n_features = train_df.columns[1:-1][sorted_corrs][:n]\n",
        "\n",
        "print(\"Top\", n, \"Features:\")\n",
        "print(top_n_features)\n",
        "\n",
        "# Update X_train and X_test with the top n features\n",
        "X_train = X_train[:, sorted_corrs[:n]]\n",
        "X_test = X_test[:, sorted_corrs[:n]]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "jfKIR45bfY-a"
      },
      "outputs": [],
      "source": [
        "# from sklearn.feature_selection import RFE\n",
        "# from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# # Initialize the estimator and RFE\n",
        "# estimator = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "# selector = RFE(estimator, n_features_to_select=10, step=1)\n",
        "\n",
        "# # Fit the selector on X_train and y_train\n",
        "# selector = selector.fit(X_train, y_train)\n",
        "\n",
        "# # Get the selected features\n",
        "# selected_features = np.array(train_df.columns[1:-1])[selector.support_]\n",
        "\n",
        "# # Apply the selector on X_test\n",
        "# X_test = selector.transform(X_test)\n",
        "\n",
        "# print(\"Selected Features:\", selected_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "VjXYzrvyvHTX"
      },
      "outputs": [],
      "source": [
        "# # Define the parameter grid to search over\n",
        "# param_grid = {\n",
        "#     'n_components': [500, 600, 700, 800, 900]\n",
        "# }\n",
        "\n",
        "# # Create a PCA object\n",
        "# pca = PCA()\n",
        "\n",
        "# # Create a GridSearchCV object\n",
        "# grid_search = GridSearchCV(estimator=pca, param_grid=param_grid, cv=5, n_jobs=-1)\n",
        "\n",
        "# # Fit the GridSearchCV object on X_train\n",
        "# grid_search.fit(X_train)\n",
        "\n",
        "# # Print the best hyperparameters\n",
        "# print(\"Best hyperparameters:\", grid_search.best_params_)\n",
        "\n",
        "# # Apply PCA with the best hyperparameters\n",
        "# best_pca = grid_search.best_estimator_\n",
        "# X_train_pca = best_pca.fit_transform(X_train)\n",
        "# print(X_train_pca.shape)\n",
        "\n",
        "# exp_var = sum(best_pca.explained_variance_ratio_ * 100)\n",
        "# print('Variance explained:', exp_var)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2ChaEfgy670",
        "outputId": "0b17cf48-58b7-4ada-9b0c-73331dab6f9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1205, 263)\n",
            "Variance explained: 99.00193815095133\n"
          ]
        }
      ],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# define number of components to keep\n",
        "# n_components = 300\n",
        "\n",
        "# initialize PCA object\n",
        "pca = PCA(n_components=0.99)\n",
        "\n",
        "# fit PCA on train_df after removing outliers\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)\n",
        "print(X_train_pca.shape)\n",
        "\n",
        "exp_var = sum(pca.explained_variance_ratio_ * 100)\n",
        "print('Variance explained:', exp_var) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "uqgwzGoHrJ3w"
      },
      "outputs": [],
      "source": [
        "# from sklearn.decomposition import PCA\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# # define number of components to keep\n",
        "# # n_components = 300\n",
        "\n",
        "# # initialize PCA object\n",
        "# pca = PCA(n_components=0.99)\n",
        "\n",
        "# # fit PCA on train_df after removing outliers\n",
        "# X_train_pca = pca.fit_transform(X_train)\n",
        "# print(X_train_pca.shape)\n",
        "\n",
        "# exp_var = sum(pca.explained_variance_ratio_ * 100)\n",
        "# print('Variance explained:', exp_var) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uw-CIN1jCprm",
        "outputId": "878d3c32-0d1e-480b-bb5c-6b763fe281e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1205, 19)\n",
            "Variance explained: 100.0\n"
          ]
        }
      ],
      "source": [
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "\n",
        "lda = LinearDiscriminantAnalysis(n_components=19)\n",
        "X_train_pca = lda.fit_transform(X_train_pca, y_train)\n",
        "X_test_pca = lda.transform(X_test_pca)\n",
        "\n",
        "print(X_train_pca.shape)\n",
        "exp_var = sum(lda.explained_variance_ratio_ * 100)\n",
        "print('Variance explained:', exp_var)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "803KKo98Cz-p"
      },
      "outputs": [],
      "source": [
        "# from sklearn.cluster import KMeans\n",
        "\n",
        "# n_clusters = 10\n",
        "\n",
        "# kmeans = KMeans(n_clusters=n_clusters, init = 'k-means++' , tol = 1e-05, random_state=42)\n",
        "\n",
        "# kmeans.fit(X_train_pca)\n",
        "\n",
        "# train_df_cluster_labels = kmeans.labels_\n",
        "\n",
        "# X_train_clustered = pd.concat([pd.DataFrame(X_train_pca), pd.DataFrame(train_df_cluster_labels, columns=[\"cluster_label\"])], axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "dLiacVJgC5rz"
      },
      "outputs": [],
      "source": [
        "# from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# le = LabelEncoder()\n",
        "# y_train_encoded = le.fit_transform(y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "FANkfxFhC72D"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X_train_clustered, y_train_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "log_reg1 = LogisticRegression(C=1, max_iter=1000, solver = 'lbfgs', multi_class = 'multinomial', penalty='l2')\n",
        "log_reg2 = LogisticRegression(C=1.25, max_iter=1000, solver = 'lbfgs', multi_class = 'multinomial', penalty='l2')\n",
        "log_reg3 = LogisticRegression(C=1.5, max_iter=1000, solver = 'lbfgs', multi_class = 'multinomial', penalty='l2')\n",
        "\n",
        "voting_clf = VotingClassifier(\n",
        "    estimators=[('lr1', log_reg1), ('lr2', log_reg2), ('lr3', log_reg3)],\n",
        "    voting='soft',\n",
        "    weights=[2, 1, 1] \n",
        ")\n",
        "\n",
        "# X_train.columns = X_train.columns.astype(str)\n",
        "# X_test.columns = X_test.columns.astype(str)\n",
        "\n",
        "# scores = cross_val_score(voting_clf, X_train, y_train, cv=15)\n",
        "\n",
        "# print(\"Cross-validation scores:\", scores)\n",
        "# print(\"Mean accuracy:\", scores.mean())\n",
        "\n",
        "voting_clf.fit(X_train_pca, y_train)\n",
        "\n",
        "y_pred = voting_clf.predict(X_test_pca)\n",
        "\n",
        "# accuracy = accuracy_score(y_test, y_pred)\n",
        "# print(\"Accuracy of the ensemble model:\", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "GtwbYbEBOkTr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Convert the predictions to a DataFrame with the correct column name\n",
        "submission_df = pd.DataFrame({'category': y_pred})\n",
        "\n",
        "# Set the value of the first row in the first column to 'Leeche_Raw'\n",
        "submission_df.loc[0, 'category'] = 'Leeche_Raw'\n",
        "\n",
        "# Add an 'ID' column with values from the 'ID' column of the test data\n",
        "submission_df['ID'] = test_df['ID']\n",
        "\n",
        "# Rearrange the columns so that 'ID' is the first column\n",
        "submission_df = submission_df[['ID', 'category']]\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "submission_df.to_csv('submission.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xrwg3ucFq5mg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "c1 = pd.read_csv(\"submission (2) (4).csv\").to_numpy()\n",
        "\n",
        "c2 = pd.read_csv(\"submission (6) (1).csv\").to_numpy()\n",
        "cnt = 0\n",
        "for i in range(c1.shape[0]):\n",
        "    if(c1[i][1]!=c2[i][1]):\n",
        "        print(c1[i],c2[i])\n",
        "        cnt += 1\n",
        "print(cnt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSs6XAwmq6wq",
        "outputId": "a61d5581-21e5-4659-b151-5f025618bbe5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[18 'Papaya_Ripe'] [18 'Mango_Ripe']\n",
            "[34 'Mango_Raw'] [34 'Coconut_Raw']\n",
            "[73 'Papaya_Ripe'] [73 'Papaya_Raw']\n",
            "[80 'Mango_Raw'] [80 'Papaya_Raw']\n",
            "[81 'Orange_Ripe'] [81 'Mango_Raw']\n",
            "[93 'Apple_Ripe'] [93 'Coconut_Ripe']\n",
            "[103 'Coconut_Ripe'] [103 'Papaya_Raw']\n",
            "[105 'Pomengranate_Ripe'] [105 'Apple_Ripe']\n",
            "[107 'Papaya_Ripe'] [107 'Mango_Ripe']\n",
            "[135 'Banana_Raw'] [135 'Orange_Ripe']\n",
            "[145 'Coconut_Raw'] [145 'Papaya_Raw']\n",
            "[161 'Papaya_Ripe'] [161 'Strawberry_Raw']\n",
            "[191 'Mango_Ripe'] [191 'Apple_Ripe']\n",
            "[211 'Papaya_Ripe'] [211 'Strawberry_Ripe']\n",
            "[221 'Coconut_Ripe'] [221 'Orange_Ripe']\n",
            "[234 'Strawberry_Raw'] [234 'Banana_Raw']\n",
            "[235 'Guava_Raw'] [235 'Papaya_Raw']\n",
            "[242 'Pomengranate_Ripe'] [242 'Leeche_Ripe']\n",
            "[250 'Orange_Raw'] [250 'Papaya_Raw']\n",
            "[260 'Coconut_Ripe'] [260 'Coconut_Raw']\n",
            "[265 'Mango_Raw'] [265 'Apple_Raw']\n",
            "[271 'Coconut_Ripe'] [271 'Apple_Raw']\n",
            "[273 'Papaya_Ripe'] [273 'Mango_Ripe']\n",
            "[276 'Mango_Ripe'] [276 'Orange_Ripe']\n",
            "[291 'Apple_Raw'] [291 'Guava_Ripe']\n",
            "[294 'Banana_Raw'] [294 'Mango_Raw']\n",
            "[317 'Strawberry_Raw'] [317 'Strawberry_Ripe']\n",
            "[329 'Coconut_Ripe'] [329 'Apple_Raw']\n",
            "[332 'Papaya_Ripe'] [332 'Papaya_Raw']\n",
            "[336 'Coconut_Raw'] [336 'Orange_Raw']\n",
            "[348 'Leeche_Ripe'] [348 'Leeche_Raw']\n",
            "[353 'Papaya_Ripe'] [353 'Papaya_Raw']\n",
            "[367 'Leeche_Ripe'] [367 'Leeche_Raw']\n",
            "[371 'Mango_Raw'] [371 'Orange_Raw']\n",
            "[378 'Apple_Raw'] [378 'Guava_Ripe']\n",
            "[390 'Banana_Ripe'] [390 'Banana_Raw']\n",
            "[391 'Mango_Ripe'] [391 'Coconut_Ripe']\n",
            "[411 'Apple_Raw'] [411 'Pomengranate_Ripe']\n",
            "38\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "c1 = pd.read_csv(\"submission.csv\").to_numpy()\n",
        "\n",
        "c2 = pd.read_csv(\"submission_62_results (1).csv\").to_numpy()\n",
        "cnt = 0\n",
        "for i in range(c1.shape[0]):\n",
        "    if(c1[i][1]!=c2[i][1]):\n",
        "        print(c1[i],c2[i])\n",
        "        cnt += 1\n",
        "print(cnt)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
