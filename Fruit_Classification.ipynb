{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwrRmhL_qkeU"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "# Load the data\n",
        "train_df = pd.read_csv(\"train.csv\")\n",
        "test_df = pd.read_csv(\"test.csv\")\n",
        "\n",
        "# Separate the target variable\n",
        "y_train = train_df[\"category\"].to_numpy() \n",
        "X_train = train_df.drop([\"ID\", \"category\"], axis=1).to_numpy() \n",
        "X_test = test_df.drop([\"ID\"], axis=1).to_numpy() \n",
        "\n",
        "# # Apply feature engineering\n",
        "# for i in range(X_train.shape[1]):\n",
        "#     col = X_train[:, i]\n",
        "#     X_train = np.column_stack((X_train, col + col ** 3 + col ** 2))\n",
        "\n",
        "# # Standardize the data using z-score normalization\n",
        "# scaler = StandardScaler()\n",
        "# X_train = scaler.fit_transform(X_train)\n",
        "\n",
        "# # Standardize the data using z-score normalization\n",
        "# scaler = StandardScaler()\n",
        "# X_train_standardized = scaler.fit_transform(X_train)\n",
        "# X_train = X_train_standardized\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the LOF algorithm on X_train\n",
        "lof = LocalOutlierFactor(n_neighbors=5, algorithm='ball_tree', metric='euclidean')\n",
        "outliers = lof.fit_predict(X_train)\n",
        "\n",
        "# Remove the outliers from X_train and y_train\n",
        "X_train = X_train[outliers == 1]\n",
        "y_train = y_train[outliers == 1]\n",
        "\n",
        "print(len(X_train))"
      ],
      "metadata": {
        "id": "b7aUdsz9qsAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the correlation matrix\n",
        "corr_matrix = np.corrcoef(X_train, rowvar=False)\n",
        "\n",
        "# Get the correlations with the target variable\n",
        "corr_with_target = corr_matrix[:-1, -1]\n",
        "\n",
        "# Get the absolute values of the correlations\n",
        "abs_corr_with_target = np.abs(corr_with_target)\n",
        "\n",
        "# Sort the correlations in descending order\n",
        "sorted_corrs = np.argsort(abs_corr_with_target)[::-1]\n",
        "\n",
        "# Get the top n features\n",
        "n = 2000\n",
        "top_n_features = train_df.columns[1:-1][sorted_corrs][:n]\n",
        "\n",
        "print(\"Top\", n, \"Features:\")\n",
        "print(top_n_features)\n",
        "\n",
        "# Update X_train and X_test with the top n features\n",
        "X_train = X_train[:, sorted_corrs[:n]]\n",
        "X_test = X_test[:, sorted_corrs[:n]]"
      ],
      "metadata": {
        "id": "1sRGDNegqvuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# define number of components to keep\n",
        "# n_components = 300\n",
        "\n",
        "# initialize PCA object\n",
        "pca = PCA(n_components=0.99)\n",
        "\n",
        "# fit PCA on train_df after removing outliers\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)\n",
        "print(X_train_pca.shape)\n",
        "\n",
        "exp_var = sum(pca.explained_variance_ratio_ * 100)\n",
        "print('Variance explained:', exp_var) "
      ],
      "metadata": {
        "id": "SO5s9VYaqwzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "\n",
        "lda = LinearDiscriminantAnalysis(n_components=19)\n",
        "X_train_pca = lda.fit_transform(X_train_pca, y_train)\n",
        "X_test_pca = lda.transform(X_test_pca)\n",
        "\n",
        "print(X_train_pca.shape)\n",
        "exp_var = sum(lda.explained_variance_ratio_ * 100)\n",
        "print('Variance explained:', exp_var)"
      ],
      "metadata": {
        "id": "ZqRwCNI_q0-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X_train_clustered, y_train_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "log_reg1 = LogisticRegression(C=1, max_iter=1000, solver = 'lbfgs', multi_class = 'multinomial', penalty='l2')\n",
        "log_reg2 = LogisticRegression(C=1.25, max_iter=1000, solver = 'lbfgs', multi_class = 'multinomial', penalty='l2')\n",
        "log_reg3 = LogisticRegression(C=1.5, max_iter=1000, solver = 'lbfgs', multi_class = 'multinomial', penalty='l2')\n",
        "\n",
        "voting_clf = VotingClassifier(\n",
        "    estimators=[('lr1', log_reg1), ('lr2', log_reg2), ('lr3', log_reg3)],\n",
        "    voting='soft',\n",
        "    weights=[2, 1, 1] \n",
        ")\n",
        "\n",
        "# X_train.columns = X_train.columns.astype(str)\n",
        "# X_test.columns = X_test.columns.astype(str)\n",
        "\n",
        "# scores = cross_val_score(voting_clf, X_train, y_train, cv=15)\n",
        "\n",
        "# print(\"Cross-validation scores:\", scores)\n",
        "# print(\"Mean accuracy:\", scores.mean())\n",
        "\n",
        "voting_clf.fit(X_train_pca, y_train)\n",
        "\n",
        "y_pred = voting_clf.predict(X_test_pca)\n",
        "\n",
        "# accuracy = accuracy_score(y_test, y_pred)\n",
        "# print(\"Accuracy of the ensemble model:\", accuracy)"
      ],
      "metadata": {
        "id": "ee7EQ_rMq5_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Convert the predictions to a DataFrame with the correct column name\n",
        "submission_df = pd.DataFrame({'category': y_pred})\n",
        "\n",
        "# Set the value of the first row in the first column to 'Leeche_Raw'\n",
        "submission_df.loc[0, 'category'] = 'Leeche_Raw'\n",
        "\n",
        "# Add an 'ID' column with values from the 'ID' column of the test data\n",
        "submission_df['ID'] = test_df['ID']\n",
        "\n",
        "# Rearrange the columns so that 'ID' is the first column\n",
        "submission_df = submission_df[['ID', 'category']]\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "submission_df.to_csv('submission.csv', index=False)\n"
      ],
      "metadata": {
        "id": "knDMLFYFq8O1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "c1 = pd.read_csv(\"submission (2) (4).csv\").to_numpy()\n",
        "\n",
        "c2 = pd.read_csv(\"submission (6) (1).csv\").to_numpy()\n",
        "cnt = 0\n",
        "for i in range(c1.shape[0]):\n",
        "    if(c1[i][1]!=c2[i][1]):\n",
        "        print(c1[i],c2[i])\n",
        "        cnt += 1\n",
        "print(cnt)\n"
      ],
      "metadata": {
        "id": "dRyO8bHKq9ra"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}